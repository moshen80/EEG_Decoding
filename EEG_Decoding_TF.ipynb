{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9aef4822-1ade-4528-b9f1-bb00322f6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462bce3-a457-483e-95bb-9f584b948c4e",
   "metadata": {},
   "source": [
    "### Reading the Training and Test data\n",
    "The dimention of the data shall be Trials x Contatcts x Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33306bae-b324-41a9-bca4-3742e49c9de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEG_data = \"Data/EEG\"\n",
    "Label_data = \"Data/Labels\"\n",
    "# Reading Training EEG Data and their labels\n",
    "train_data = pd.read_csv(os.path.join(EEG_data,'training_set.csv'), header=None)\n",
    "train_data = np.array(train_data).astype('float32')\n",
    "\n",
    "# Reading Test EEG Data and their labels\n",
    "test_data = pd.read_csv(os.path.join(EEG_data, 'test_set.csv'), header=None)\n",
    "test_data = np.array(test_data).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84e481fc-de14-407f-83c2-e5eef2e943e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(os.path.join(Label_data , 'training_label.csv'), header=None)\n",
    "train_labels = np.array(train_labels).astype('int')\n",
    "test_labels = pd.read_csv(os.path.join(Label_data,'test_label.csv'), header=None)\n",
    "test_labels = np.array(test_labels).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1d34c-8c07-4c88-92b2-6e4a0e2a970f",
   "metadata": {},
   "source": [
    "Both Training and Test Data have 64x64 feature size which come from 64-contacts EEG signal and 64 sample in time \\\n",
    "which will be used for classification purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c1e4b63-52d0-4f8c-994f-cf6fc029f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size: (76356, 4096)\n",
      "Test Data Size: (8484, 4096)\n"
     ]
    }
   ],
   "source": [
    "print('Train Data Size: {}'.format(train_data.shape))\n",
    "print('Test Data Size: {}'.format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69832c62-6c18-4e6f-8b4d-cbf055df5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all the stack and use GPU resources as much as possible\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf361e97-714d-442c-aef5-378cef422469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot representation of trian and test labels\n",
    "train_labels = tf.one_hot(indices=train_labels, depth=4)\n",
    "train_labels = tf.squeeze(train_labels).eval(Session=sess)\n",
    "test_labels = tf.one_hot(indices=test_labels, depth=4)\n",
    "test_labels = tf.squeeze(test_labels).eval(Session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d892480-bc69-4cb0-8c63-a9fa0ee58ce2",
   "metadata": {},
   "source": [
    "### Initializing the Model Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e55de480-3e5b-424e-9832-73b3b3a4ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_contacts   = 64      # The number of EEG contacts at each time point\n",
    "n_time  = 64      # number of EEG time points to feed to the model\n",
    "n_neurons_lstm = 256     # number of neurons in the LSTM layn_er\n",
    "n_attention = 8  # The number of neurons in attention layer\n",
    "\n",
    "n_class   = 4     # The number of classification classes\n",
    "n_neurons_FC  = 64    # The number of hidden units in the FC layer\n",
    "num_epoch = 300   # The number of Epochs that the Model run\n",
    "keep_rate = 0.75  # Keep rate of the Dropout\n",
    "\n",
    "lr = tf.constant(1e-4, dtype=tf.float32)  # Learning rate\n",
    "lr_decay_epoch = 50    # Every (50) epochs, the learning rate decays\n",
    "lr_decay       = 0.50  # Learning rate Decay by (50%)\n",
    "\n",
    "batch_size = 128\n",
    "n_batch = train_data.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f12ba-26c8-407b-94a8-b716bd823172",
   "metadata": {},
   "source": [
    "### Initializing the Weights and Biases and creating placeholders for Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db83a1-1fff-4928-9c9d-7d1148adada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the weights using normal distributed small random numbers\n",
    "W_1 = tf.Variable(tf.truncated_normal([2 * n_neurons_lstm, n_neurons_FC], stddev=0.01))\n",
    "b_1  = tf.Variable(tf.constant(0.01, shape=[n_neurons_FC]))\n",
    "W_2 = tf.Variable(tf.truncated_normal([n_neurons_FC, n_class], stddev=0.01))\n",
    "b_2  = tf.Variable(tf.constant(0.01, shape=[n_class]))\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 64 * 64])\n",
    "y = tf.placeholder(tf.float32, [None, 4])\n",
    "dropout_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03728fdb-a1bb-425c-822d-b1012aa8525d",
   "metadata": {},
   "source": [
    "### Defining Loss and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9ecc4-5d79-4289-af49-a9e4a30ab729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_l2(y, y_pred,l2_norm):\n",
    "    train_variable = tf.trainable_variables()\n",
    "    regularization_loss = l2_norm * tf.reduce_sum([tf.nn.l2_loss(v) for v in train_variable])\n",
    "    model_loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "    loss = tf.reduce_mean(model_loss + regularization_loss)\n",
    "    return loss\n",
    "\n",
    "def evaluation(y, y_pred):\n",
    "    y_corr = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "    Global_Average_Accuracy = tf.reduce_mean(tf.cast(y_corr, tf.float32))\n",
    "    return Global_Average_Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52572d28-4865-4f3b-b324-ef40906195fd",
   "metadata": {},
   "source": [
    "### Creating the Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5954e29-5d23-4de3-a864-a5286e7072b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention_Layer(inputs, n_attention):\n",
    "    inputs = tf.concat(inputs, 2)\n",
    "    n_lstm = inputs.shape[2].value\n",
    "\n",
    "    w_omega = tf.Variable(tf.random_normal([n_lstm, n_attention], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([n_attention], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([n_attention], stddev=0.1))\n",
    "\n",
    "    with tf.name_scope('v'):\n",
    "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
    "\n",
    "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')\n",
    "    alphas = tf.nn.softmax(vu, name='alphas')\n",
    "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff37ecb-0f73-4784-9d1f-9da10fdd759e",
   "metadata": {},
   "source": [
    "### Creating the Main Model\n",
    "the main model is the combination of Bidirectional LSTM, Attention Mechanism, Fully Connected and Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "545e7346-75ce-451f-bc9f-d82e9b657d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiLSTM_Attention_FC_Sofmax(Input, n_time, n_contacts, n_neurons_lstm, n_attention, keep_prob,\n",
    "                          W_1, b_1, W_2, b_2):\n",
    "\n",
    "    Input = tf.reshape(Input, [-1, n_time, n_contacts])\n",
    "\n",
    "    forward_lstm = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons_lstm,activation='tanh')\n",
    "    backward_lstm = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons_lstm,activation='tanh')\n",
    "\n",
    "    lstm_fw_drop = tf.contrib.rnn.DropoutWrapper(cell=forward_lstm, input_keep_prob=keep_prob)\n",
    "    lstm_bw_drop = tf.contrib.rnn.DropoutWrapper(cell=backward_lstm, input_keep_prob=keep_prob)\n",
    "\n",
    "    outputs, _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(lstm_fw_drop, lstm_bw_drop, Input, dtype=tf.float32)\n",
    "\n",
    "    attention_output = Attention_Layer(outputs,n_attention)\n",
    "    attention_output_drop = tf.nn.dropout(attention_output, keep_prob)\n",
    "\n",
    "    FC_1 = tf.matmul(attention_output_drop, W_1) + b_1\n",
    "    FC_1 = tf.layers.batch_normalization(FC_1, training=True)\n",
    "    FC_1 = tf.nn.softplus(FC_1)\n",
    "    FC_1 = tf.nn.dropout(FC_1, keep_prob)\n",
    "\n",
    "    FC_2 = tf.matmul(FC_1, W_2) + b_2\n",
    "    \n",
    "    output = tf.nn.softmax(FC_2)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9e130d-4a1a-491e-a4f9-a177b304e555",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d992159d-2f11-4c78-8e99-e96f087e394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = BiLSTM_Attention_FC_Sofmax(Input=X,n_time=n_time,n_neurons_lstm=n_neurons_lstm,\n",
    "                                n_contacts=n_contacts,n_attention=n_attention,keep_prob=dropout_prob,\n",
    "                                W_1=W_1,b_1=b_1,W_2=W_2,b_2=b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bebdf7-ee1d-43fa-af71-5685851419bb",
   "metadata": {},
   "source": [
    "### Loading Loss Function, Optimizer and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad278a73-1e3e-4dcf-b488-931748740ba1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = loss_l2(y=y, y_pred=y_pred,l2_norm=0.001)\n",
    "Optimizer = tf.train.AdamOptimizer(lr).minimize(loss_fn)\n",
    "Global_Average_Accuracy = evaluation(y=y, y_pred=y_pred)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d9dfc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize all the variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c7a9c",
   "metadata": {},
   "source": [
    "### Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223c385",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "\n",
    "    # Decaying the learning rate\n",
    "    learning_rate = sess.run(lr)\n",
    "    if epoch % lr_decay_epoch == 0 and epoch != 0:\n",
    "        if learning_rate > 1e-6:\n",
    "            lr = lr * lr_decay\n",
    "            sess.run(lr)            \n",
    "\n",
    "    for batch_index in range(n_batch):\n",
    "        random_batch = random.sample(range(train_data.shape[0]), batch_size)\n",
    "        batch_xs = train_data[random_batch]\n",
    "        batch_ys = train_labels[random_batch]\n",
    "        sess.run(Optimizer, feed_dict={X: batch_xs, y: batch_ys, dropout_prob: keep_rate})\n",
    "\n",
    "\n",
    "    train_accuracy, train_loss = sess.run([Global_Average_Accuracy, loss_fn], feed_dict={X: train_data[0:100], y: train_labels[0:100], keep_prob: 1.0})\n",
    "    Test_summary, test_accuracy, test_loss = sess.run([merged, Global_Average_Accuracy, loss_fn],\n",
    "                                                      feed_dict={X: test_data, y: test_labels, dropout_prob: 1.0})\n",
    "\n",
    "    # Print Model Accuracy\n",
    "    print(\"Iter \" + str(epoch) + \", Testing Accuracy: \" + str(test_accuracy) + \", Training Accuracy: \" + str(train_accuracy))\n",
    "    print(\"Iter \" + str(epoch) + \", Testing Loss: \" + str(test_loss) + \", Training Loss: \" + str(train_loss))\n",
    "    print(\"Learning rate is \", learning_rate)\n",
    "    print('\\n')\n",
    "\n",
    "    if epoch == num_epoch:\n",
    "        output_prediction = sess.run(y_pred, feed_dict={X: test_data, y: test_labels, dropout_prob: 1.0})\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
